{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pandas.io.parsers import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
    "from keras import models\n",
    "import keras_tuner as kt\n",
    "from keras_tuner.tuners import Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(validation_split):\n",
    "\n",
    "    # Load data from csv file into data frame, drop all rows that have missing values\n",
    "    df = read_csv(\"training.csv\")\n",
    "    print(df[\"Image\"].count())\n",
    "    df = df.dropna()\n",
    "    print(df[\"Image\"].count())\n",
    "\n",
    "    # Convert the rows of the image column from pixel values separated by spaces to numpy arrays\n",
    "    df[\"Image\"] = df[\"Image\"].apply(lambda img: np.fromstring(img, sep=\" \"))\n",
    "\n",
    "    # Create numpy matrix from image column by stacking the rows vertically\n",
    "    X_data = np.vstack(df[\"Image\"].values)\n",
    "    # Normalize pixel values to (0, 1) range\n",
    "    X_data = X_data / 255\n",
    "    # Convert to float32, which is the default for Keras\n",
    "    X_data = X_data.astype(\"float32\")\n",
    "    # Reshape each row from one dimensional arrays to (height, width, num_channels) = (96, 96, 1)\n",
    "    X_data = X_data.reshape(-1, 96, 96, 1)\n",
    "    # Extract labels representing the coordinates of facial landmarks\n",
    "    Y_data = df[df.columns[:-1]].values\n",
    "\n",
    "    # Normalize coordinates to (0, 1) range\n",
    "    Y_data = Y_data / 96\n",
    "    Y_data = Y_data.astype(\"float32\")\n",
    "\n",
    "    # Shuffle data\n",
    "    X_data, Y_data = shuffle(X_data, Y_data)\n",
    "\n",
    "    # Split data into training set and validation set\n",
    "    split_index = int(X_data.shape[0] * (1 - validation_split))\n",
    "    X_train = X_data[:split_index]\n",
    "    Y_train = Y_data[:split_index]\n",
    "    X_val = X_data[split_index:]\n",
    "    Y_val = Y_data[split_index:]\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7049\n",
      "2140\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_val, Y_val = load_data(validation_split=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(hp):\n",
    "    \"\"\"function to to generate a combination of hyperparameter to use in RandomSearch\n",
    "\n",
    "    Args:\n",
    "        hp: object of cominations\n",
    "\n",
    "    Returns:\n",
    "        _type_: Sequential model\n",
    "    \"\"\"\n",
    "    \n",
    "    filters = hp.Choice('num_filters', values=[32, 64], default=64)\n",
    "    units = hp.Int('units', min_value=32, max_value=512, step=32, default=128)\n",
    "    activation = hp.Choice('dense_activation', values=['relu', 'tanh', 'sigmoid'], default='relu' )\n",
    "    rate = hp.Float('learning_rate', min_value=0.0001, max_value=0.01, sampling='LOG', default=0.001)\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        \n",
    "        Conv2D(filters=filters, kernel_size=(5,5), input_shape=(96, 96, 1), activation=activation, padding=\"same\"),\n",
    "        MaxPool2D(pool_size=(2, 2)),\n",
    "        Conv2D(filters=filters, kernel_size=(5,5), activation=activation, padding=\"same\"),\n",
    "        MaxPool2D(pool_size=(2, 2)),\n",
    "        Conv2D(filters=filters, kernel_size=(5,5), activation=activation, padding=\"same\"),\n",
    "        Dropout(rate=rate),\n",
    "        MaxPool2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(units=units, activation=activation),\n",
    "        Dense(30)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Hyperband(\n",
    "    create_cnn_model,\n",
    "    max_epochs=10,\n",
    "    objective='val_loss',\n",
    "    seed=1,\n",
    "    executions_per_trial=2,\n",
    "    directory='hyperband',\n",
    "    project_name='cifar10'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 4\n",
      "num_filters (Choice)\n",
      "{'default': 64, 'conditions': [], 'values': [32, 64], 'ordered': True}\n",
      "units (Int)\n",
      "{'default': 128, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "dense_activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'sigmoid'], 'ordered': False}\n",
      "learning_rate (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 04m 52s]\n",
      "val_loss: 0.0005298343603499234\n",
      "\n",
      "Best val_loss So Far: 0.0004672050126828253\n",
      "Total elapsed time: 01h 40m 49s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, Y_train,  epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cnn_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 96, 96, 64)        1664      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 48, 48, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 64)        102464    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 24, 24, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 64)        102464    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 480)               4424160   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 30)                14430     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,645,182\n",
      "Trainable params: 4,645,182\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 96, 96, 64)        1664      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 48, 48, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 64)        102464    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 24, 24, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 64)        102464    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 480)               4424160   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 30)                14430     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,645,182\n",
      "Trainable params: 4,645,182\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 2s 126ms/step - loss: 4.1400e-04\n"
     ]
    }
   ],
   "source": [
    "cnn_val_loss = best_cnn_model.evaluate(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00041399820474907756"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_val_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(Flatten(input_shape=(96, 96, 1)))\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dense(30))\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 1s 76ms/step - loss: 16.5986 - val_loss: 0.5971\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 0.3469 - val_loss: 0.2472\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 0.2346 - val_loss: 0.2154\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 0.1986 - val_loss: 0.1750\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 0.1571 - val_loss: 0.1336\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 0.1175 - val_loss: 0.0975\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 0.0842 - val_loss: 0.0693\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 0.0594 - val_loss: 0.0495\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 0.0428 - val_loss: 0.0361\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 0.0311 - val_loss: 0.0278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x229c1736c20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model = create_baseline_model()\n",
    "baseline_model.fit(X_train, Y_train, batch_size=200, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 5ms/step - loss: 0.0268\n"
     ]
    }
   ],
   "source": [
    "final_val_loss = baseline_model.evaluate(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026807500049471855"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_cnn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\landmark_detection\\landmark_detection.ipynb Cell 19\u001b[0m in \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/landmark_detection/landmark_detection.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(\u001b[39m'\u001b[39m\u001b[39msaved_models\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/landmark_detection/landmark_detection.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Save CNN model weights\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/landmark_detection/landmark_detection.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m best_cnn_model\u001b[39m.\u001b[39msave_weights(\u001b[39m\"\u001b[39m\u001b[39msaved_models/cnn_model_weights.h5\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/landmark_detection/landmark_detection.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Save CNN model architecture\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/landmark_detection/landmark_detection.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39msaved_models/cnn_model_architecture.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_cnn_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Create directory if it does not exist\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "# Save CNN model weights\n",
    "best_cnn_model.save_weights(\"saved_models/cnn_model_weights.h5\")\n",
    "\n",
    "# Save CNN model architecture\n",
    "with open(\"saved_models/cnn_model_architecture.json\", \"w\") as f:\n",
    "         f.write(best_cnn_model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save baseline model weights\n",
    "baseline_model.save_weights(\"saved_models/baseline_model_weights.h5\")\n",
    "\n",
    "# save baseline model architecture\n",
    "with open(\"saved_models/baseline_model_architecture.json\", \"w\") as f:\n",
    "    f.write(baseline_model.to_json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(y_pred, img_size_x, img_size_y):\n",
    "    landmarks = []\n",
    "    for i in range(0, len(y_pred), 2):\n",
    "        landmark_x, landmark_y = y_pred[i] * img_size_x, y_pred[i+1] * img_size_y\n",
    "        landmarks.append((landmark_x, landmark_y))\n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name: str):\n",
    "    with open(\"saved_models/\" + model_name + \"_model_architecture.json\", \"r\") as f:\n",
    "        model = models.model_from_json(f.read())\n",
    "    model.load_weights(\"saved_models/\" + model_name + \"_model_weights.h5\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img_with_landmarks(img, landmarks, filename):\n",
    "    # Convert image to grayscale\n",
    "    img_gray = np.mean(img, axis=-1)\n",
    "\n",
    "    # Plot image and landmarks\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img_gray, cmap='gray')\n",
    "    ax.scatter([landmark[0] for landmark in landmarks], \n",
    "               [landmark[1] for landmark in landmarks], \n",
    "               marker='o', s=10, c='g')\n",
    "\n",
    "    # Save image\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sunglasses_filter(model_name:str='cnn', filter_name:str='sunglasses'):\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Convert frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect faces\n",
    "        face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        \n",
    "        # Crop face\n",
    "        if len(faces) > 0:\n",
    "            (x,y,w,h) = faces[0]\n",
    "            gray = gray[y:y+h, x:x+w]\n",
    "            frame = frame[y:y+h, x:x+w]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Resize face\n",
    "        gray = cv2.resize(gray, dsize=(96, 96), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Convert frame to RGB format\n",
    "        orig_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        orig_size_x, orig_size_y = orig_img.shape[0], orig_img.shape[1]\n",
    "\n",
    "        # Prepare input image\n",
    "        img = np.expand_dims(gray, axis=2)\n",
    "        img = img.astype(\"float32\") / 255\n",
    "\n",
    "        # Predict landmarks\n",
    "        model = load_model(model_name)\n",
    "        y_pred = model.predict(np.expand_dims(img, axis=0))[0]\n",
    "        landmarks = extract_landmarks(y_pred, orig_size_x, orig_size_y)\n",
    "        \n",
    "        # Save original image with landmarks on top\n",
    "        save_img_with_landmarks(orig_img, landmarks, \"landmarks.png\")\n",
    "        # Extract x and y values from landmarks of interest\n",
    "        # eye position\n",
    "        left_eye_center_x = int(landmarks[0][0])\n",
    "        left_eye_center_y = int(landmarks[0][1])\n",
    "        right_eye_center_x = int(landmarks[1][0])\n",
    "        right_eye_center_y = int(landmarks[1][1])\n",
    "        left_eye_outer_x = int(landmarks[3][0])\n",
    "        right_eye_outer_x = int(landmarks[5][0])\n",
    "\n",
    "        # Load images using PIL\n",
    "        # PIL has better functions for rotating and pasting compared to cv2\n",
    "        face_img = Image.fromarray(frame)\n",
    "        sunglasses_img = Image.open(filter_name + \".png\")\n",
    "\n",
    "        # Resize sunglasses\n",
    "        sunglasses_width = int((left_eye_outer_x - right_eye_outer_x) * 1.4)\n",
    "        sunglasses_height = int(sunglasses_img.size[1] * (sunglasses_width / sunglasses_img.size[0]))\n",
    "        sunglasses_resized = sunglasses_img.resize((sunglasses_width, sunglasses_height))\n",
    "\n",
    "        # Rotate sunglasses\n",
    "        eye_angle_radians = np.arctan((right_eye_center_y - left_eye_center_y) / (left_eye_center_x - right_eye_center_x))\n",
    "        sunglasses_rotated = sunglasses_resized.rotate(np.degrees(eye_angle_radians), expand=True, resample=Image.BICUBIC)\n",
    "\n",
    "        # Compute positions such that the center of the sunglasses is\n",
    "        # positioned at the center point between the eyes\n",
    "        x_offset = int(sunglasses_width * 0.5)\n",
    "        y_offset = int(sunglasses_height * 0.5)\n",
    "        pos_x = int((left_eye_center_x + right_eye_center_x) / 2)\n",
    "        pos_y = int((left_eye_center_y + right_eye_center_y) / 2) - y_offset\n",
    "\n",
    "        # Paste sunglasses on face image\n",
    "        face_img.paste(sunglasses_rotated, (pos_x, pos_y), sunglasses_rotated)\n",
    "\n",
    "        # Convert image back to OpenCV format\n",
    "        final_img = cv2.cvtColor(np.array(face_img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Display the resulting image\n",
    "        cv2.imshow('frame',final_img)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('o'):\n",
    "                break\n",
    "                \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 77ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danar\\AppData\\Local\\Temp\\ipykernel_9912\\517795988.py:63: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  sunglasses_rotated = sunglasses_resized.rotate(np.degrees(eye_angle_radians), expand=True, resample=Image.BICUBIC)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n"
     ]
    }
   ],
   "source": [
    "sunglasses_filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
